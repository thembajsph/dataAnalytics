{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beyond Random Forests: More Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hide warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "elec_data = fetch_openml(name='electricity', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the whole dataframe\n",
    "\n",
    "elec_df = elec_data.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training data is: 33984 \n",
      "The size of testing data is: 11328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(elec_df , test_size=0.25,random_state=20)\n",
    "\n",
    "print('The size of training data is: {} \\nThe size of testing data is: {}'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>period</th>\n",
       "      <th>nswprice</th>\n",
       "      <th>nswdemand</th>\n",
       "      <th>vicprice</th>\n",
       "      <th>vicdemand</th>\n",
       "      <th>transfer</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27325</th>\n",
       "      <td>0.469846</td>\n",
       "      <td>4</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.164705</td>\n",
       "      <td>0.519637</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.657949</td>\n",
       "      <td>0.265789</td>\n",
       "      <td>DOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28731</th>\n",
       "      <td>0.474227</td>\n",
       "      <td>5</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.024919</td>\n",
       "      <td>0.191907</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.090886</td>\n",
       "      <td>0.819737</td>\n",
       "      <td>DOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8450</th>\n",
       "      <td>0.023141</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.065270</td>\n",
       "      <td>0.250074</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>DOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36659</th>\n",
       "      <td>0.889385</td>\n",
       "      <td>2</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.148193</td>\n",
       "      <td>0.670039</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>0.533402</td>\n",
       "      <td>0.563596</td>\n",
       "      <td>UP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>0.000708</td>\n",
       "      <td>4</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.124204</td>\n",
       "      <td>0.475454</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>UP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13013</th>\n",
       "      <td>0.428963</td>\n",
       "      <td>7</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.055242</td>\n",
       "      <td>0.084647</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>DOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.009203</td>\n",
       "      <td>1</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.741892</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>DOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18851</th>\n",
       "      <td>0.446662</td>\n",
       "      <td>2</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.183409</td>\n",
       "      <td>0.785034</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>0.757639</td>\n",
       "      <td>0.517105</td>\n",
       "      <td>UP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14838</th>\n",
       "      <td>0.433830</td>\n",
       "      <td>3</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.047886</td>\n",
       "      <td>0.141476</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.422915</td>\n",
       "      <td>0.414912</td>\n",
       "      <td>DOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30462</th>\n",
       "      <td>0.868236</td>\n",
       "      <td>6</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.030833</td>\n",
       "      <td>0.702023</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.538322</td>\n",
       "      <td>0.674123</td>\n",
       "      <td>UP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date day    period  nswprice  nswdemand  vicprice  vicdemand  \\\n",
       "27325  0.469846   4  0.276596  0.164705   0.519637  0.011417   0.657949   \n",
       "28731  0.474227   5  0.574468  0.024919   0.191907  0.001656   0.090886   \n",
       "8450   0.023141   3  0.042553  0.065270   0.250074  0.003467   0.422915   \n",
       "36659  0.889385   2  0.744681  0.148193   0.670039  0.009981   0.533402   \n",
       "781    0.000708   4  0.276596  0.124204   0.475454  0.003467   0.422915   \n",
       "13013  0.428963   7  0.106383  0.055242   0.084647  0.003467   0.422915   \n",
       "3330   0.009203   1  0.382979  0.045635   0.741892  0.003467   0.422915   \n",
       "18851  0.446662   2  0.744681  0.183409   0.785034  0.012154   0.757639   \n",
       "14838  0.433830   3  0.127660  0.047886   0.141476  0.003467   0.422915   \n",
       "30462  0.868236   6  0.638298  0.030833   0.702023  0.001963   0.538322   \n",
       "\n",
       "       transfer class  \n",
       "27325  0.265789  DOWN  \n",
       "28731  0.819737  DOWN  \n",
       "8450   0.414912  DOWN  \n",
       "36659  0.563596    UP  \n",
       "781    0.414912    UP  \n",
       "13013  0.414912  DOWN  \n",
       "3330   0.414912  DOWN  \n",
       "18851  0.517105    UP  \n",
       "14838  0.414912  DOWN  \n",
       "30462  0.674123    UP  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('class', axis=1)\n",
    "y_train = train_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "y_train_prepared = label_enc.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_classifier =  LogisticRegression()\n",
    "sv_classifier = SVC()\n",
    "sgd_classifier = SGDClassifier()\n",
    "\n",
    "\n",
    "def classifiers(clf1, clf2, clf3, X_train, y_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function that takes 5 inputs: 3 classifiers, training data & labels\n",
    "    And return the list of accuracies on all classifiers\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # A list of all classifiers\n",
    "    clfs = [clf1, clf2, clf3]\n",
    "    \n",
    "    # An empty list to comprehend \n",
    "    all_clfs_acc = []\n",
    "    \n",
    "    # Train each classifier, evaluate it on the training set \n",
    "    # And append the accuracy to 'all_clfs_acc' \n",
    "    \n",
    "    for clf in clfs:\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_train)\n",
    "        acc = accuracy_score(y_train,preds)\n",
    "        acc = acc.tolist()\n",
    "        all_clfs_acc.append(acc)\n",
    "        \n",
    "    return all_clfs_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.751088747645951, 0.7368761770244822, 0.7454390301318268]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers(log_classifier,sv_classifier, sgd_classifier, X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, the function returned 4 accuracies on the training set. The first accuracy correspond to Logistic Regression, the second is Support Vector Classifier, and the third is SGD(Stockastic Gradient Descent,\n",
    "\n",
    "# Now, let us use Voting Classifier to aggregate the results of all of those 3 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log_reg', LogisticRegression()), ('svc', SVC()),\n",
       "                             ('sgd', SGDClassifier())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vot_classifier = VotingClassifier(\n",
    "    \n",
    "    estimators=[('log_reg', log_classifier),\n",
    "                ('svc', sv_classifier),\n",
    "                ('sgd', sgd_classifier)], \n",
    "    voting='hard')\n",
    "\n",
    "vot_classifier.fit(X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(model, data, labels):\n",
    "    \n",
    "    predictions = model.predict(data)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced'),\n",
       "                  bootstrap=False, max_features=0.5, max_samples=0.5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_classifier = BaggingClassifier(\n",
    "      DecisionTreeClassifier(class_weight='balanced'),\n",
    "    max_samples=0.5, max_features=0.5, bootstrap=False\n",
    ")\n",
    "\n",
    "bag_classifier.fit(X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798140301318268"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(bag_classifier, X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.8, max_depth=2, n_estimators=500,\n",
       "                           random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_boost_clf = GradientBoostingClassifier(\n",
    "                        n_estimators=500, \n",
    "                        learning_rate=0.8, \n",
    "                        random_state=42,\n",
    "                        max_depth=2)\n",
    "\n",
    "grad_boost_clf.fit(X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9269362052730696"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(grad_boost_clf, X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " #AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced',\n",
       "                                                         max_depth=3),\n",
       "                   learning_rate=0.5, n_estimators=300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=3, class_weight='balanced'), \n",
    "    #base estimator is decision trees by default\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.5\n",
    "\n",
    ")\n",
    "\n",
    "adaboost_clf.fit(X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93717631826742"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(adaboost_clf, X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('rand',\n",
       "                                RandomForestClassifier(random_state=42)),\n",
       "                               ('svc', SVC(random_state=42))],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "base_estimators = [\n",
    "    ('rand', RandomForestClassifier(random_state=42)),\n",
    "    ('svc', SVC(random_state=42))]\n",
    "\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "stack_clf = StackingClassifier(estimators = base_estimators, \n",
    "                               final_estimator = final_estimator)\n",
    "\n",
    "stack_clf.fit(X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996468926553672"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(stack_clf, X_train, y_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or categorical.  When\n                categorical type is supplied, DMatrix parameter\n                `enable_categorical` must be set to `True`.day",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7fd89bacfd66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# model = xgboost.XGBClassifier()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/anaconda/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/anaconda/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         train_dmatrix, evals = self._wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mfeature_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/anaconda/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(self, X, y, group, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, eval_group, label_transform)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         train_dmatrix = DMatrix(data=X, label=y, weight=sample_weight,\n\u001b[0m\u001b[1;32m    266\u001b[0m                                 \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                                 missing=self.missing, nthread=self.n_jobs)\n",
      "\u001b[0;32m~/Downloads/anaconda/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, enable_categorical)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch_data_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         handle, feature_names, feature_types = dispatch_data_backend(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/anaconda/lib/python3.8/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         return _from_pandas_df(data, enable_categorical, missing, threads,\n\u001b[0m\u001b[1;32m    540\u001b[0m                                feature_names, feature_types)\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/anaconda/lib/python3.8/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    240\u001b[0m def _from_pandas_df(data, enable_categorical, missing, nthread,\n\u001b[1;32m    241\u001b[0m                     feature_names, feature_types):\n\u001b[0;32m--> 242\u001b[0;31m     data, feature_names, feature_types = _transform_pandas_df(\n\u001b[0m\u001b[1;32m    243\u001b[0m         data, enable_categorical, feature_names, feature_types)\n\u001b[1;32m    244\u001b[0m     return _from_numpy_array(data, missing, nthread, feature_names,\n",
      "\u001b[0;32m~/Downloads/anaconda/lib/python3.8/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mcategorical\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msupplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDMatrix\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 `enable_categorical` must be set to `True`.\"\"\"\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or categorical.  When\n                categorical type is supplied, DMatrix parameter\n                `enable_categorical` must be set to `True`.day"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train_prepared)\n",
    "# print(xgb_clf)\n",
    "# # model = xgboost.XGBClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Ensemble Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop('class', axis=1)\n",
    "y_test = test_data['class']\n",
    "\n",
    "y_test_prepared = label_enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8968926553672316"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(grad_boost_clf, X_test, y_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also evaluate the stacking classifier. It was overly optimistic on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9021892655367232"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(stack_clf, X_test, y_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How about trying a bag classifier also? It had nearly 98% on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875882768361582"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(bag_classifier, X_test, y_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
