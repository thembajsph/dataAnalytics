{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intro to NLP and Text Processing with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Encoding with Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample sentence\n",
    "\n",
    "sentences = ['TensorFlow is a Machine Learning framework',\n",
    "             'Keras is a well designed deep learning API',\n",
    "             'TensorFlow and Keras make a great machine learning ecosystem'\n",
    "              \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TensorFlow is a Machine Learning framework',\n",
       " 'Keras is a well designed deep learning API',\n",
       " 'TensorFlow and Keras make a great machine learning ecosystem']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1000, char_level=True)\n",
    "\n",
    "# Fitting tokenizer on sentences\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 'a': 3, 'n': 4, 'r': 5, 's': 6, 'i': 7, 'l': 8, 'o': 9, 'm': 10, 'g': 11, 't': 12, 'w': 13, 'k': 14, 'd': 15, 'f': 16, 'c': 17, 'h': 18, 'p': 19, 'y': 20}\n"
     ]
    }
   ],
   "source": [
    "char_index = tokenizer.word_index\n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'learning': 2, 'tensorflow': 3, 'is': 4, 'machine': 5, 'keras': 6, 'framework': 7, 'well': 8, 'designed': 9, 'deep': 10, 'api': 11, 'and': 12, 'make': 13, 'great': 14, 'ecosystem': 15}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# Fitting tokenizer on sentences\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tensorflow', 2),\n",
       "             ('is', 2),\n",
       "             ('a', 3),\n",
       "             ('machine', 2),\n",
       "             ('learning', 3),\n",
       "             ('framework', 1),\n",
       "             ('keras', 2),\n",
       "             ('well', 1),\n",
       "             ('designed', 1),\n",
       "             ('deep', 1),\n",
       "             ('api', 1),\n",
       "             ('and', 1),\n",
       "             ('make', 1),\n",
       "             ('great', 1),\n",
       "             ('ecosystem', 1)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = tokenizer.word_counts\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Converting the Texts into Sequence of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TensorFlow is a Machine Learning framework',\n",
       " 'Keras is a well designed deep learning API',\n",
       " 'TensorFlow and Keras make a great machine learning ecosystem']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Redefining our sentence\n",
    "\n",
    "sentences = ['TensorFlow is a Machine Learning framework',\n",
    "             'Keras is a well designed deep learning API',\n",
    "             'TensorFlow and Keras make a great machine learning ecosystem!',\n",
    "             'TensorFlow is built on top of Keras',\n",
    "             'TensorFlow revolves around tensors!'\n",
    "              \n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Converting text to sequences\n",
    "\n",
    "text_sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with tokens: {'tensorflow': 1, 'is': 2, 'a': 3, 'learning': 4, 'keras': 5, 'machine': 6, 'framework': 7, 'well': 8, 'designed': 9, 'deep': 10, 'api': 11, 'and': 12, 'make': 13, 'great': 14, 'ecosystem': 15, 'built': 16, 'on': 17, 'top': 18, 'of': 19, 'revolves': 20, 'around': 21, 'tensors': 22}\n",
      "Sequence of tokens: [[1, 2, 3, 6, 4, 7], [5, 2, 3, 8, 9, 10, 4, 11], [1, 12, 5, 13, 3, 14, 6, 4, 15], [1, 2, 16, 17, 18, 19, 5], [1, 20, 21, 22]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Words with tokens: {word_index}')\n",
    "print(f'Sequence of tokens: {text_sequences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with tokens: {'Word Out of Vocab': 1, 'tensorflow': 2, 'is': 3, 'a': 4, 'learning': 5, 'keras': 6, 'machine': 7, 'framework': 8, 'well': 9, 'designed': 10, 'deep': 11, 'api': 12, 'and': 13, 'make': 14, 'great': 15, 'ecosystem': 16, 'built': 17, 'on': 18, 'top': 19, 'of': 20, 'revolves': 21, 'around': 22, 'tensors': 23}\n",
      "Sequence of tokens: [[2, 3, 4, 7, 5, 8], [6, 3, 4, 9, 10, 11, 5, 12], [2, 13, 6, 14, 4, 15, 7, 5, 16], [2, 3, 17, 18, 19, 20, 6], [2, 21, 22, 23]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=1000, oov_token='Word Out of Vocab')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Converting text to sequences\n",
    "\n",
    "text_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(f'Words with tokens: {word_index}')\n",
    "print(f'Sequence of tokens: {text_sequences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence of tokens: [[1, 1, 2], [6, 3, 4, 1, 11, 5, 12]]\n"
     ]
    }
   ],
   "source": [
    "new_sentences = ['I like TensorFlow', # like is a new word\n",
    "                'Keras is a superb deep learning API' # superb is a new word\n",
    "                \n",
    "] \n",
    "\n",
    "sequences_on_newtexts = tokenizer.texts_to_sequences(new_sentences)\n",
    "print(f'Sequence of tokens: {sequences_on_newtexts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Padding the Sequences to Have the Same Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD INDEX\n",
      "Words with tokens: {'Word Out of Vocab': 1, 'tensorflow': 2, 'is': 3, 'a': 4, 'learning': 5, 'keras': 6, 'machine': 7, 'framework': 8, 'well': 9, 'designed': 10, 'deep': 11, 'api': 12, 'and': 13, 'make': 14, 'great': 15, 'ecosystem': 16, 'built': 17, 'on': 18, 'top': 19, 'of': 20, 'revolves': 21, 'around': 22, 'tensors': 23}\n",
      "---------\n",
      "SEQUENCES\n",
      "Words with tokens: [[2, 3, 4, 7, 5, 8], [6, 3, 4, 9, 10, 11, 5, 12], [2, 13, 6, 14, 4, 15, 7, 5, 16], [2, 3, 17, 18, 19, 20, 6], [2, 21, 22, 23]]\n",
      "---------\n",
      "PADDED SEQUENCES\n",
      "Sequence of tokens: [[ 0  0  0  0  2  3  4  7  5  8]\n",
      " [ 0  0  6  3  4  9 10 11  5 12]\n",
      " [ 0  2 13  6 14  4 15  7  5 16]\n",
      " [ 0  0  0  2  3 17 18 19 20  6]\n",
      " [ 0  0  0  0  0  0  2 21 22 23]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Padding text_sequences \n",
    "\n",
    "padded_sequences = pad_sequences(text_sequences, maxlen=10)\n",
    "\n",
    "\n",
    "print('WORD INDEX')\n",
    "print(f'Words with tokens: {word_index}')\n",
    "print(\"---------\")\n",
    "\n",
    "\n",
    "print('SEQUENCES')\n",
    "print(f'Words with tokens: {text_sequences}')\n",
    "print(\"---------\")\n",
    "\n",
    "print('PADDED SEQUENCES')\n",
    "print(f'Sequence of tokens: {padded_sequences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADDED SEQUENCES\n",
      "Sequence of tokens: [[ 2  3  4  7  5  8 -1 -1 -1 -1]\n",
      " [ 6  3  4  9 10 11  5 12 -1 -1]\n",
      " [ 2 13  6 14  4 15  7  5 16 -1]\n",
      " [ 2  3 17 18 19 20  6 -1 -1 -1]\n",
      " [ 2 21 22 23 -1 -1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(text_sequences, maxlen=10, padding='post', value=-1)\n",
    "\n",
    "print('PADDED SEQUENCES')\n",
    "print(f'Sequence of tokens: {padded_sequences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using TextVectorization Layer to Preprocess Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "\n",
    "sentences = [\n",
    "             'TensorFlow is a deep learning library!',\n",
    "             'Is TensorFlow powered by Keras API?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers' has no attribute 'TextVectorization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9dbc99956ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m text_vect_layer = tf.keras.layers.TextVectorization(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'TextVectorization'"
     ]
    }
   ],
   "source": [
    "max_features = 1000\n",
    "\n",
    "text_vect_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=10\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (2.4.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 497.6 MB 119 bytes/s  0:00:01    |██                              | 32.4 MB 2.4 MB/s eta 0:03:18     |█████▌                          | 86.2 MB 1.1 MB/s eta 0:06:11     |██████▏                         | 95.8 MB 217 kB/s eta 0:30:45     |███████▉                        | 121.0 MB 1.0 MB/s eta 0:06:08     |████████▏                       | 127.2 MB 892 kB/s eta 0:06:56     |████████▊                       | 135.0 MB 746 kB/s eta 0:08:06     |██████████▎                     | 159.8 MB 1.4 MB/s eta 0:04:08     |██████████▍                     | 160.8 MB 1.4 MB/s eta 0:04:08     |████████████████▏               | 251.5 MB 858 kB/s eta 0:04:47     |████████████████████▏           | 313.3 MB 1.2 MB/s eta 0:02:33     |████████████████████▌           | 319.3 MB 1.3 MB/s eta 0:02:22     |█████████████████████▍          | 332.9 MB 1.4 MB/s eta 0:01:59     |██████████████████████▉         | 355.3 MB 1.1 MB/s eta 0:02:05     |███████████████████████████████ | 480.4 MB 512 kB/s eta 0:00:34     |███████████████████████████████ | 482.6 MB 1.4 MB/s eta 0:00:11\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/tf-estimator-nightly/\u001b[0m\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (3.15.7)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 1.4 MB/s eta 0:00:01    |████████████████████            | 9.0 MB 481 kB/s eta 0:00:12\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 240 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.22.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.8 MB 98 kB/s  eta 0:00:01     |█████████████████████           | 11.0 MB 771 kB/s eta 0:00:08\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/thembajsph/.local/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 38 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 544 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.28.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/thembajsph/Downloads/anaconda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tensorboard-data-server, numpy, tf-estimator-nightly, tensorflow-io-gcs-filesystem, tensorboard, libclang, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.4.1\n",
      "    Uninstalling tensorflow-2.4.1:\n",
      "      Successfully uninstalled tensorflow-2.4.1\n",
      "Successfully installed keras-2.8.0 libclang-13.0.0 numpy-1.22.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.23.1 tf-estimator-nightly-2.8.0.dev2021122109\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the '/home/thembajsph/Downloads/anaconda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_vect_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5a2ca26b61e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_vect_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_vect_layer' is not defined"
     ]
    }
   ],
   "source": [
    "text_vect_layer.adapt(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_vect_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7971dd3ec290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Tensorflow is a machine learning framework!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvectorized_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_vect_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Orginal sentence: \\n {sample_sentence}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_vect_layer' is not defined"
     ]
    }
   ],
   "source": [
    "sample_sentence = 'Tensorflow is a machine learning framework!'\n",
    "\n",
    "vectorized_sentence = text_vect_layer([sample_sentence])\n",
    "\n",
    "print(f'Orginal sentence: \\n {sample_sentence}')\n",
    "print(f'Vectorized sentence: \\n {vectorized_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
